import os
from dotenv import load_dotenv
import openai
from typing import List

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
LLM_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")

# ðŸ§± HeuristickÃ© rozdÄ›lenÃ­ po 2 odstavcÃ­ch s pÅ™ekryvem
def split_text_heuristically(text: str, chunk_size: int = 2, overlap: float = 0.3) -> List[str]:
    paragraphs = [p.strip() for p in text.split("\n") if p.strip()]
    if not paragraphs:
        return []

    step = max(1, int(chunk_size * (1 - overlap)))
    chunks = []

    for i in range(0, len(paragraphs), step):
        chunk = "\n".join(paragraphs[i:i + chunk_size])
        if chunk:
            chunks.append(chunk.strip())

    return chunks

# ðŸ¤– Pokud chunk vypadÃ¡ podezÅ™ele, poÅ¡leme ho do LLM pro lepÅ¡Ã­ rozdÄ›lenÃ­
def llm_refine_chunk(text: str) -> List[str]:
    prompt = f"""
RozdÄ›l nÃ¡sledujÃ­cÃ­ text na kratÅ¡Ã­ ucelenÃ© bloky (chunky) tak, aby kaÅ¾dÃ½ obsahoval logicky propojenÃ© informace. KaÅ¾dÃ½ blok mÅ¯Å¾e obsahovat 1â€“3 odstavce. PÅ™idej 1â€“2 vÄ›ty pÅ™ekryvu mezi bloky, pokud je to vhodnÃ©.

VraÅ¥ pouze seznam chunkÅ¯ jako JSON pole Å™etÄ›zcÅ¯.

Text:
{text.strip()}
"""
    try:
        response = openai.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.4
        )
        raw_output = response.choices[0].message.content.strip()
        chunks = eval(raw_output)  # quick & dirty â€“ mÅ¯Å¾eÅ¡ nahradit json.loads pokud vÃ½stup bude JSON string
        if isinstance(chunks, list) and all(isinstance(c, str) for c in chunks):
            return chunks
    except Exception as e:
        print(f"âŒ LLM refine error: {e}")
    return [text.strip()]

# ðŸ”€ Kombinace obou pÅ™Ã­stupÅ¯
def split_text_smart(text: str, chunk_size: int = 2, overlap: float = 0.3) -> List[str]:
    initial_chunks = split_text_heuristically(text, chunk_size, overlap)
    final_chunks = []

    for chunk in initial_chunks:
        num_lines = chunk.count("\n")
        length = len(chunk)

        # PodezÅ™elÃ© chunk-y: pÅ™Ã­liÅ¡ dlouhÃ©, pÅ™Ã­liÅ¡ krÃ¡tkÃ© nebo bez odstavcÅ¯
        if length > 1800 or length < 200 or num_lines < 1:
            print("âš ï¸ Refining chunk pomocÃ­ LLM...")
            refined = llm_refine_chunk(chunk)
            final_chunks.extend(refined)
        else:
            final_chunks.append(chunk)

    return final_chunks
